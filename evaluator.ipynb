{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxBzO7diShlG5sybG5j8S5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raycmarange/AML430_CAPSTONE_PROJECT/blob/main/evaluator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.py - ENHANCED PERFORMANCE VERSION\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional\n",
        "from typing import Dict, List, Any, Optional\n",
        "from model_architectures import AdvancedModelFactory\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
        "import warnings\n",
        "from model_architectures import AdvancedModelFactory\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class UnifiedModelEvaluator:\n",
        "    \"\"\"Enhanced evaluator with correlation monitoring\"\"\"\n",
        "\n",
        "    def __init__(self, model, test_loader, feature_names, device):\n",
        "        self.model = model\n",
        "        self.test_loader = test_loader\n",
        "        self.feature_names = feature_names\n",
        "        self.device = device\n",
        "        self.results = {}\n",
        "\n",
        "\n",
        "    def get(self, key, default=None):\n",
        "        \"\"\"Dictionary-like get method for compatibility\"\"\"\n",
        "        return self.results.get(key, default)\n",
        "\n",
        "    def investigate_lstm_stress_performance(self, model_name):\n",
        "        \"\"\"Temporary alias for enhanced_stress_analysis\"\"\"\n",
        "        return self.enhanced_stress_analysis()\n",
        "\n",
        "\n",
        "    def enhanced_stress_analysis(self):\n",
        "        \"\"\"Enhanced stress analysis with robust error handling\"\"\"\n",
        "        try:\n",
        "            if not self.results.get('regime_performance'):\n",
        "                # Calculate regime performance if not already done\n",
        "                self._ensure_regime_performance()\n",
        "\n",
        "            regime_perf = self.results['regime_performance']\n",
        "\n",
        "            analysis = {\n",
        "                'stress_periods_analyzed': True,\n",
        "                'normal_regime_performance': regime_perf.get('Normal', {}),\n",
        "                'stress_regime_performance': regime_perf.get('Stress', {}),\n",
        "                'performance_gap': self._calculate_performance_gap(regime_perf),\n",
        "                'robustness_score': self._calculate_robustness_score(regime_perf)\n",
        "            }\n",
        "\n",
        "            return analysis\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Enhanced stress analysis failed: {e}\")\n",
        "            return {\n",
        "                'stress_periods_analyzed': False,\n",
        "                'error': str(e),\n",
        "                'normal_regime_performance': {'mse': 0.01, 'direction_accuracy': 0.5},\n",
        "                'stress_regime_performance': {'mse': 0.01, 'direction_accuracy': 0.5}\n",
        "            }\n",
        "\n",
        "    def _ensure_regime_performance(self):\n",
        "        \"\"\"Ensure regime performance is calculated\"\"\"\n",
        "        if 'regime_performance' not in self.results:\n",
        "            # Create dummy regime data and calculate\n",
        "            print(\"üîÑ Calculating regime performance...\")\n",
        "            self.model.eval()\n",
        "\n",
        "            all_preds, all_targets_reg, all_regimes = [], [], []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in self.test_loader:\n",
        "                    if len(batch) == 6:\n",
        "                        features, reg_target_6m, class_target_6m, reg_target_1m, class_target_1m, regime = batch\n",
        "                    elif len(batch) == 4:\n",
        "                        features, reg_target_6m, class_target_6m, regime = batch\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    features = features.to(self.device)\n",
        "                    model_output = self.model(features)\n",
        "                    reg_predictions = model_output[0].cpu().numpy().flatten()\n",
        "\n",
        "                    all_preds.extend(reg_predictions)\n",
        "                    all_targets_reg.extend(reg_target_6m.cpu().numpy().flatten())\n",
        "                    all_regimes.extend(regime.cpu().numpy().flatten())\n",
        "\n",
        "            true_returns = np.array(all_targets_reg)\n",
        "            pred_returns = np.array(all_preds)\n",
        "            regimes = np.array(all_regimes)\n",
        "\n",
        "            self.results['regime_performance'] = self._analyze_regime_performance(true_returns, pred_returns, regimes)\n",
        "\n",
        "    def _calculate_performance_gap(self, regime_perf):\n",
        "        \"\"\"Calculate performance gap between normal and stress periods\"\"\"\n",
        "        try:\n",
        "            normal_acc = regime_perf.get('Normal', {}).get('direction_accuracy', 0.5)\n",
        "            stress_acc = regime_perf.get('Stress', {}).get('direction_accuracy', 0.5)\n",
        "\n",
        "            normal_mse = regime_perf.get('Normal', {}).get('mse', 0.01)\n",
        "            stress_mse = regime_perf.get('Stress', {}).get('mse', 0.01)\n",
        "\n",
        "            accuracy_gap = abs(normal_acc - stress_acc)\n",
        "            mse_gap = abs(stress_mse - normal_mse) / normal_mse if normal_mse > 0 else 0\n",
        "\n",
        "            return {\n",
        "                'accuracy_gap': accuracy_gap,\n",
        "                'mse_ratio': stress_mse / normal_mse if normal_mse > 0 else 1.0,\n",
        "                'interpretation': 'Minimal degradation' if accuracy_gap < 0.1 and mse_gap < 0.5 else 'Significant degradation'\n",
        "            }\n",
        "        except:\n",
        "            return {'accuracy_gap': 0, 'mse_ratio': 1.0, 'interpretation': 'Unknown'}\n",
        "\n",
        "    def _calculate_robustness_score(self, regime_perf):\n",
        "        \"\"\"Calculate robustness score (0-1) for stress periods\"\"\"\n",
        "        try:\n",
        "            normal_acc = regime_perf.get('Normal', {}).get('direction_accuracy', 0.5)\n",
        "            stress_acc = regime_perf.get('Stress', {}).get('direction_accuracy', 0.5)\n",
        "\n",
        "            # Score based on performance retention during stress\n",
        "            if normal_acc > 0:\n",
        "                retention_ratio = stress_acc / normal_acc\n",
        "            else:\n",
        "                retention_ratio = 0\n",
        "\n",
        "            # Cap at 1.0 and ensure minimum score\n",
        "            robustness = min(1.0, retention_ratio)\n",
        "            robustness = max(0.0, robustness)  # Ensure non-negative\n",
        "\n",
        "            return {\n",
        "                'score': robustness,\n",
        "                'grade': 'Excellent' if robustness > 0.8 else 'Good' if robustness > 0.6 else 'Fair' if robustness > 0.4 else 'Poor'\n",
        "            }\n",
        "        except:\n",
        "            return {'score': 0.5, 'grade': 'Unknown'}\n",
        "    def _calculate_prediction_correlation(self, true_returns, pred_returns):\n",
        "        \"\"\"Calculate prediction-actual correlation\"\"\"\n",
        "        if len(true_returns) > 1:\n",
        "            correlation = np.corrcoef(true_returns, pred_returns)[0, 1]\n",
        "            return correlation\n",
        "        return 0.0\n",
        "\n",
        "    def evaluate_model(self, return_predictions=False):\n",
        "        \"\"\"Enhanced evaluation with correlation tracking\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        all_preds, all_targets_reg, all_regimes = [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.test_loader:\n",
        "                if len(batch) == 6:\n",
        "                    features, reg_target_6m, class_target_6m, reg_target_1m, class_target_1m, regime = batch\n",
        "                elif len(batch) == 4:\n",
        "                    features, reg_target_6m, class_target_6m, regime = batch\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected batch size: {len(batch)}\")\n",
        "\n",
        "                features = features.to(self.device)\n",
        "\n",
        "                # FIX: Remove return_uncertainty parameter - models don't support it\n",
        "                model_output = self.model(features)  # Remove return_uncertainty=False\n",
        "                reg_predictions = model_output[0].cpu().numpy().flatten()\n",
        "\n",
        "                all_preds.extend(reg_predictions)\n",
        "                all_targets_reg.extend(reg_target_6m.cpu().numpy().flatten())\n",
        "                all_regimes.extend(regime.cpu().numpy().flatten())\n",
        "\n",
        "        # Calculate metrics\n",
        "        true_returns = np.array(all_targets_reg)\n",
        "        pred_returns = np.array(all_preds)\n",
        "\n",
        "        metrics = self._calculate_all_metrics(true_returns, pred_returns)\n",
        "\n",
        "        # üÜï ADD CORRELATION TO METRICS\n",
        "        correlation = self._calculate_prediction_correlation(true_returns, pred_returns)\n",
        "        metrics['prediction_correlation'] = correlation\n",
        "\n",
        "        # Enhanced stress analysis - FIX: Ensure regime_performance is always set\n",
        "        regime_performance = self._analyze_regime_performance(true_returns, pred_returns, np.array(all_regimes))\n",
        "        self.results['regime_performance'] = regime_performance\n",
        "\n",
        "        predictions_dict = {\n",
        "            'regression_true': true_returns,\n",
        "            'regression_pred': pred_returns\n",
        "        }\n",
        "\n",
        "        return metrics, predictions_dict\n",
        "\n",
        "    def _calculate_all_metrics(self, true_returns, pred_returns):\n",
        "        \"\"\"Calculate comprehensive metrics\"\"\"\n",
        "        mse = mean_squared_error(true_returns, pred_returns)\n",
        "        mae = mean_absolute_error(true_returns, pred_returns)\n",
        "\n",
        "        # Direction accuracy\n",
        "        true_dir = (true_returns > 0).astype(int)\n",
        "        pred_dir = (pred_returns > 0).astype(int)\n",
        "        dir_acc = accuracy_score(true_dir, pred_dir)\n",
        "\n",
        "        # Sharpe ratio\n",
        "        strategy_returns = []\n",
        "        for pred_r, true_r in zip(pred_returns, true_returns):\n",
        "            if pred_r > 0:\n",
        "                strategy_returns.append(true_r)\n",
        "            else:\n",
        "                strategy_returns.append(0.0)\n",
        "\n",
        "        strategy_returns = np.array(strategy_returns)\n",
        "        risk_free_rate_daily = 0.02 / 252\n",
        "        excess_returns = strategy_returns - risk_free_rate_daily\n",
        "\n",
        "        std_dev = np.std(excess_returns)\n",
        "        if std_dev < 1e-8:\n",
        "            annualized_sharpe = 0.0\n",
        "        else:\n",
        "            sharpe = np.mean(excess_returns) / std_dev\n",
        "            annualized_sharpe = sharpe * np.sqrt(252)\n",
        "\n",
        "        return {\n",
        "            'mse': mse,\n",
        "            'mae': mae,\n",
        "            'direction_accuracy': dir_acc,\n",
        "            'sharpe_ratio': annualized_sharpe,\n",
        "            'prediction_correlation': 0.0  # Will be calculated separately\n",
        "        }\n",
        "\n",
        "    def _analyze_regime_performance(self, true_returns, pred_returns, regimes):\n",
        "        \"\"\"Robust regime performance analysis with comprehensive error handling\"\"\"\n",
        "        try:\n",
        "            if len(np.unique(regimes)) < 2:\n",
        "                print(\"‚ö†Ô∏è Only one regime found in test data\")\n",
        "                return {\n",
        "                    'Normal': {'mse': 0.01, 'direction_accuracy': 0.5, 'samples': len(true_returns)},\n",
        "                    'Stress': {'mse': 0.01, 'direction_accuracy': 0.5, 'samples': 0}\n",
        "                }\n",
        "\n",
        "            normal_mask = regimes == 0\n",
        "            stress_mask = regimes == 1\n",
        "\n",
        "            # Calculate MSE for each regime\n",
        "            normal_mse = mean_squared_error(true_returns[normal_mask], pred_returns[normal_mask]) if np.sum(normal_mask) > 10 else 0.01\n",
        "            stress_mse = mean_squared_error(true_returns[stress_mask], pred_returns[stress_mask]) if np.sum(stress_mask) > 10 else 0.01\n",
        "\n",
        "            # Calculate direction accuracy\n",
        "            normal_dir_acc = accuracy_score(\n",
        "                (true_returns[normal_mask] > 0).astype(int),\n",
        "                (pred_returns[normal_mask] > 0).astype(int)\n",
        "            ) if np.sum(normal_mask) > 10 else 0.5\n",
        "\n",
        "            stress_dir_acc = accuracy_score(\n",
        "                (true_returns[stress_mask] > 0).astype(int),\n",
        "                (pred_returns[stress_mask] > 0).astype(int)\n",
        "            ) if np.sum(stress_mask) > 10 else 0.5\n",
        "\n",
        "            print(f\"üìä Regime Analysis - Normal: {np.sum(normal_mask)} samples, Stress: {np.sum(stress_mask)} samples\")\n",
        "            print(f\"   ‚Ä¢ Normal - MSE: {normal_mse:.4f}, Dir Acc: {normal_dir_acc:.3f}\")\n",
        "            print(f\"   ‚Ä¢ Stress - MSE: {stress_mse:.4f}, Dir Acc: {stress_dir_acc:.3f}\")\n",
        "\n",
        "            return {\n",
        "                'Normal': {\n",
        "                    'mse': normal_mse,\n",
        "                    'direction_accuracy': normal_dir_acc,\n",
        "                    'samples': np.sum(normal_mask)\n",
        "                },\n",
        "                'Stress': {\n",
        "                    'mse': stress_mse,\n",
        "                    'direction_accuracy': stress_dir_acc,\n",
        "                    'samples': np.sum(stress_mask)\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Regime performance analysis failed: {e}\")\n",
        "            return {\n",
        "                'Normal': {'mse': 0.01, 'direction_accuracy': 0.5, 'samples': 0},\n",
        "                'Stress': {'mse': 0.01, 'direction_accuracy': 0.5, 'samples': 0}\n",
        "            }\n",
        "\n",
        "    def enhanced_stress_analysis(self):\n",
        "        \"\"\"Enhanced stress analysis with robust error handling - FIXED VERSION\"\"\"\n",
        "        try:\n",
        "            # Ensure we have the basic evaluation results first\n",
        "            if not self.results:\n",
        "                print(\"üîÑ Running initial evaluation for stress analysis...\")\n",
        "                self.evaluate_model()\n",
        "\n",
        "            # Get regime performance if available\n",
        "            regime_perf = self.results.get('regime_performance', {})\n",
        "\n",
        "            if not regime_perf:\n",
        "                print(\"üîÑ Calculating regime performance...\")\n",
        "                # Calculate regime performance if not already done\n",
        "                self.model.eval()\n",
        "\n",
        "                all_preds, all_targets_reg, all_regimes = [], [], []\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for batch in self.test_loader:\n",
        "                        if len(batch) == 6:\n",
        "                            features, reg_target_6m, class_target_6m, reg_target_1m, class_target_1m, regime = batch\n",
        "                        elif len(batch) == 4:\n",
        "                            features, reg_target_6m, class_target_6m, regime = batch\n",
        "                        else:\n",
        "                            continue\n",
        "\n",
        "                        features = features.to(self.device)\n",
        "                        model_output = self.model(features)\n",
        "\n",
        "                        # Extract regression predictions (first output)\n",
        "                        if isinstance(model_output, (list, tuple)):\n",
        "                            reg_predictions = model_output[0]\n",
        "                        else:\n",
        "                            reg_predictions = model_output\n",
        "\n",
        "                        reg_predictions = reg_predictions.cpu().numpy().flatten()\n",
        "\n",
        "                        all_preds.extend(reg_predictions)\n",
        "                        all_targets_reg.extend(reg_target_6m.cpu().numpy().flatten())\n",
        "                        all_regimes.extend(regime.cpu().numpy().flatten())\n",
        "\n",
        "                if len(all_preds) == 0:\n",
        "                    return {\n",
        "                        'stress_periods_analyzed': False,\n",
        "                        'error': 'No predictions generated'\n",
        "                    }\n",
        "\n",
        "                true_returns = np.array(all_targets_reg)\n",
        "                pred_returns = np.array(all_preds)\n",
        "                regimes = np.array(all_regimes)\n",
        "\n",
        "                regime_perf = self._analyze_regime_performance(true_returns, pred_returns, regimes)\n",
        "                self.results['regime_performance'] = regime_perf\n",
        "\n",
        "            # Now perform the enhanced analysis\n",
        "            analysis = {\n",
        "                'stress_periods_analyzed': True,\n",
        "                'normal_regime_performance': regime_perf.get('Normal', {}),\n",
        "                'stress_regime_performance': regime_perf.get('Stress', {}),\n",
        "                'performance_gap': self._calculate_performance_gap(regime_perf),\n",
        "                'robustness_score': self._calculate_robustness_score(regime_perf)\n",
        "            }\n",
        "\n",
        "            return analysis\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Enhanced stress analysis failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {\n",
        "                'stress_periods_analyzed': False,\n",
        "                'error': str(e),\n",
        "                'normal_regime_performance': {'mse': 0.01, 'direction_accuracy': 0.5},\n",
        "                'stress_regime_performance': {'mse': 0.01, 'direction_accuracy': 0.5}\n",
        "            }\n",
        "\n",
        "    def plot_performance_analysis(self):\n",
        "        \"\"\"Plot performance analysis\"\"\"\n",
        "        print(\"üìä Performance analysis plotted (Stub - implement as needed)\")\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"Generate evaluation report\"\"\"\n",
        "        print(\"üìã Generating evaluation report (Stub - implement as needed)\")\n",
        "\n",
        "    def _get_fallback_metrics(self):\n",
        "        \"\"\"Get fallback metrics in case of failure\"\"\"\n",
        "        return {\n",
        "            'mse': 1.0, 'mae': 1.0, 'direction_accuracy': 0.5,\n",
        "            'sharpe_ratio': 0.0, 'prediction_correlation': 0.0\n",
        "        }\n",
        "\n",
        "class AdvancedRegimeAwareTrainer:\n",
        "    \"\"\"Advanced trainer with comprehensive performance optimizations\"\"\"\n",
        "\n",
        "    def __init__(self, model, train_loader, val_loader, config, model_type=None):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.config = config\n",
        "        self.model_type = model_type or model.__class__.__name__\n",
        "\n",
        "        # Enhanced directories\n",
        "        self.LOG_DIR = \"./logs/\"\n",
        "        self.MODEL_DIR = \"./saved_models/\"\n",
        "        self.create_directories()\n",
        "\n",
        "        # Advanced experiment tracking\n",
        "        self.experiment_name = f\"{self.model_type}_advanced_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        self.experiment_dir = os.path.join(self.LOG_DIR, self.experiment_name)\n",
        "        os.makedirs(self.experiment_dir, exist_ok=True)\n",
        "\n",
        "        # Enhanced loss functions with multiple options\n",
        "        self.regression_criterion = self._select_regression_criterion()\n",
        "        self.classification_criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing\n",
        "\n",
        "        # Advanced optimizer configuration\n",
        "        self.optimizer = self._create_advanced_optimizer()\n",
        "\n",
        "        # Enhanced learning rate schedulers\n",
        "        self.schedulers = self._create_advanced_schedulers()\n",
        "\n",
        "        # Advanced training state\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.early_stopping_counter = 0\n",
        "        # üÜï FIX: Use configurable patience\n",
        "        self.patience = getattr(config, 'EARLY_STOPPING_PATIENCE', 15)\n",
        "\n",
        "        # Comprehensive metrics tracking\n",
        "        self.metrics_history = {\n",
        "            'train_losses': [], 'val_losses': [],\n",
        "            'train_reg_losses': [], 'val_reg_losses': [],\n",
        "            'train_class_losses': [], 'val_class_losses': [],\n",
        "            'learning_rates': [], 'gradient_norms': [],\n",
        "            'train_direction_accuracy': [], 'val_direction_accuracy': [],\n",
        "            'regime_performance': {'stress': [], 'normal': []}\n",
        "        }\n",
        "\n",
        "        # Advanced performance tracking\n",
        "        self.best_metrics = {\n",
        "            'val_loss': float('inf'),\n",
        "            'val_mse': float('inf'),\n",
        "            'val_mae': float('inf'),\n",
        "            'val_direction_accuracy': 0.0,\n",
        "            'val_sharpe_ratio': 0.0\n",
        "        }\n",
        "\n",
        "        # Dynamic stress weighting\n",
        "        self.current_stress_weight = getattr(config, 'STRESS_WEIGHT', 2.0)\n",
        "        self.adaptive_weighting = True\n",
        "\n",
        "        # Gradient accumulation\n",
        "        self.gradient_accumulation_steps = getattr(config, 'GRADIENT_ACCUMULATION_STEPS', 1)\n",
        "        self.accumulation_counter = 0\n",
        "\n",
        "        # Device configuration\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Mixed precision training\n",
        "        self.scaler = torch.cuda.amp.GradScaler() if self.device.type == 'cuda' else None\n",
        "\n",
        "        print(f\"üöÄ Advanced training on device: {self.device}\")\n",
        "        print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "        print(f\"üìÅ Experiment directory: {self.experiment_dir}\")\n",
        "        print(f\"üéØ Model type: {self.model_type}\")\n",
        "        print(f\"‚ö° Mixed precision: {self.scaler is not None}\")\n",
        "        print(f\"üîÑ Gradient accumulation: {self.gradient_accumulation_steps} steps\")\n",
        "\n",
        "    def _calculate_strategy_returns(self, predictions, targets):\n",
        "        \"\"\"Calculate strategy returns based on predictions\"\"\"\n",
        "        try:\n",
        "            strategy_returns = []\n",
        "            for pred_r, true_r in zip(predictions, targets):\n",
        "                if pred_r > 0:  # Long when prediction is positive\n",
        "                    strategy_returns.append(true_r)\n",
        "                else:  # No position when prediction is negative\n",
        "                    strategy_returns.append(0.0)\n",
        "\n",
        "            return np.array(strategy_returns)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Strategy returns calculation failed: {e}\")\n",
        "            return np.zeros_like(predictions)\n",
        "\n",
        "    def _get_fallback_metrics(self):\n",
        "        \"\"\"Get fallback metrics in case of failure\"\"\"\n",
        "        return {\n",
        "            'mse': float('inf'),\n",
        "            'mae': float('inf'),\n",
        "            'sharpe_ratio': 0.0\n",
        "        }\n",
        "\n",
        "\n",
        "    def calculate_additional_metrics(self, predictions, targets):\n",
        "        \"\"\"Enhanced metrics with benchmark comparison - FIXED VERSION\"\"\"\n",
        "        try:\n",
        "            predictions_np = predictions.numpy().flatten()\n",
        "            targets_np = targets.numpy().flatten()\n",
        "\n",
        "            # Existing metrics\n",
        "            metrics = {\n",
        "                'mse': np.mean((predictions_np - targets_np) ** 2),\n",
        "                'mae': np.mean(np.abs(predictions_np - targets_np)),\n",
        "            }\n",
        "\n",
        "            # NEW: Benchmark comparison using existing Sharpe calculation - FIXED\n",
        "            buy_hold_returns = targets_np  # Buy-and-hold strategy\n",
        "            # FIX: Pass both returns and predictions to _calculate_sharpe_ratio\n",
        "            bh_sharpe = self._calculate_sharpe_ratio(buy_hold_returns, np.ones_like(buy_hold_returns))  # Always long for buy-hold\n",
        "            strategy_sharpe = self._calculate_sharpe_ratio(targets_np, predictions_np)\n",
        "\n",
        "            metrics['buy_hold_sharpe'] = bh_sharpe\n",
        "            metrics['strategy_sharpe'] = strategy_sharpe\n",
        "            metrics['sharpe_outperformance'] = strategy_sharpe - bh_sharpe\n",
        "\n",
        "            return metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Enhanced metrics calculation failed: {e}\")\n",
        "            return self._get_fallback_metrics()\n",
        "\n",
        "    def _select_regression_criterion(self):\n",
        "        \"\"\"Select appropriate regression criterion based on model type\"\"\"\n",
        "        if self.model_type == 'linear':\n",
        "            return nn.MSELoss()  # More stable for linear models\n",
        "        else:\n",
        "            return nn.HuberLoss()  # More robust for neural networks\n",
        "\n",
        "    def _create_advanced_optimizer(self):\n",
        "        \"\"\"Create advanced optimizer with model-specific settings and stronger regularization (Weight Decay)\"\"\"\n",
        "        if hasattr(self.config, 'MODEL_CONFIGS') and self.model_type in self.config.MODEL_CONFIGS:\n",
        "            model_config = self.config.MODEL_CONFIGS[self.model_type]\n",
        "            lr = model_config.get('learning_rate', self.config.LEARNING_RATE)\n",
        "            # üÜï FIX: Stronger Regularization - Weight Decay\n",
        "            weight_decay = model_config.get('weight_decay', 1e-3)\n",
        "        else:\n",
        "            lr = self.config.LEARNING_RATE\n",
        "            weight_decay = 1e-3\n",
        "\n",
        "        # Model-specific optimizer configurations\n",
        "        if 'transformer' in self.model_type.lower():\n",
        "            return optim.AdamW(\n",
        "                self.model.parameters(),\n",
        "                lr=lr,\n",
        "                # üÜï FIX: Apply Weight Decay\n",
        "                weight_decay=weight_decay,\n",
        "                betas=(0.9, 0.999),\n",
        "                eps=1e-8\n",
        "            )\n",
        "        elif 'lstm' in self.model_type.lower():\n",
        "            return optim.Adam(\n",
        "                self.model.parameters(),\n",
        "                lr=lr,\n",
        "                # üÜï FIX: Apply Weight Decay\n",
        "                weight_decay=weight_decay,\n",
        "                betas=(0.9, 0.999)\n",
        "            )\n",
        "        else:\n",
        "            return optim.AdamW(\n",
        "                self.model.parameters(),\n",
        "                lr=lr,\n",
        "                # üÜï FIX: Apply Weight Decay\n",
        "                weight_decay=weight_decay\n",
        "            )\n",
        "    def _calculate_sharpe_ratio(self, returns, predictions, risk_free_rate=0.02):\n",
        "        \"\"\"\n",
        "        Calculate Sharpe ratio for model predictions vs actual returns\n",
        "        Annualized assuming 252 trading days\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Calculate excess returns\n",
        "            if len(returns) == 0 or len(predictions) == 0:\n",
        "                return 0.0\n",
        "\n",
        "            returns = np.array(returns)\n",
        "            predictions = np.array(predictions)\n",
        "\n",
        "            # Strategy returns: long when prediction positive, short when negative\n",
        "            strategy_returns = np.where(predictions > 0, returns, -returns)\n",
        "\n",
        "            # Calculate daily risk-free rate\n",
        "            daily_rf = risk_free_rate / 252\n",
        "\n",
        "            # Calculate excess returns\n",
        "            excess_returns = strategy_returns - daily_rf\n",
        "\n",
        "            # Annualize\n",
        "            if len(excess_returns) > 1:\n",
        "                sharpe_ratio = np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(252)\n",
        "            else:\n",
        "                sharpe_ratio = 0.0\n",
        "\n",
        "            return float(sharpe_ratio)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Sharpe ratio calculation failed: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _enhanced_metrics_calculation(self, predictions, targets, returns):\n",
        "        \"\"\"\n",
        "        Comprehensive metrics calculation for training evaluation - FIXED VERSION\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        try:\n",
        "            # Sharpe Ratio - FIXED: Pass both returns and predictions\n",
        "            metrics['sharpe_ratio'] = self._calculate_sharpe_ratio(returns, predictions)\n",
        "\n",
        "            # Direction Accuracy\n",
        "            pred_direction = (predictions > 0)\n",
        "            actual_direction = (targets > 0)\n",
        "            metrics['direction_accuracy'] = np.mean(pred_direction == actual_direction)\n",
        "\n",
        "            # Additional risk-adjusted metrics\n",
        "            metrics['max_drawdown'] = self._calculate_max_drawdown(returns, predictions)\n",
        "            metrics['calmar_ratio'] = self._calculate_calmar_ratio(returns, predictions)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Enhanced metrics calculation failed: {e}\")\n",
        "            metrics = {\n",
        "                'sharpe_ratio': 0.0,\n",
        "                'direction_accuracy': 0.0,\n",
        "                'max_drawdown': 0.0,\n",
        "                'calmar_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _calculate_max_drawdown(self, returns, predictions):\n",
        "        \"\"\"Calculate maximum drawdown for strategy\"\"\"\n",
        "        try:\n",
        "            strategy_returns = np.where(predictions > 0, returns, -returns)\n",
        "            cumulative = np.cumprod(1 + strategy_returns)\n",
        "            running_max = np.maximum.accumulate(cumulative)\n",
        "            drawdown = (cumulative - running_max) / running_max\n",
        "            return float(np.min(drawdown))\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_calmar_ratio(self, returns, predictions):\n",
        "        \"\"\"Calculate Calmar ratio (return / max drawdown)\"\"\"\n",
        "        try:\n",
        "            strategy_returns = np.where(predictions > 0, returns, -returns)\n",
        "            annual_return = np.mean(strategy_returns) * 252\n",
        "            max_dd = abs(self._calculate_max_drawdown(returns, predictions))\n",
        "            return annual_return / max_dd if max_dd > 0 else 0.0\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _calibrate_confidence(self, historical_data, current_predictions, market_regime=\"normal\"):\n",
        "        \"\"\"\n",
        "        Calibrate forecast confidence using historical performance and market regime\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if historical_data is None or len(historical_data) == 0:\n",
        "                return 0.1  # Default low confidence\n",
        "\n",
        "            # Extract historical predictions and actuals\n",
        "            hist_preds = historical_data.get('predictions', [])\n",
        "            hist_actuals = historical_data.get('actuals', [])\n",
        "            hist_regimes = historical_data.get('regimes', [])\n",
        "\n",
        "            if len(hist_preds) < 10:  # Insufficient history\n",
        "                return 0.1\n",
        "\n",
        "            # Calculate historical accuracy by regime\n",
        "            regime_accuracies = {}\n",
        "            for regime in ['normal', 'stress']:\n",
        "                regime_mask = [r == regime for r in hist_regimes] if hist_regimes else [True] * len(hist_preds)\n",
        "\n",
        "                if sum(regime_mask) > 5:  # Minimum samples\n",
        "                    regime_preds = hist_preds[regime_mask]\n",
        "                    regime_actuals = hist_actuals[regime_mask]\n",
        "\n",
        "                    # Direction accuracy\n",
        "                    correct_directions = np.sum(\n",
        "                        (regime_preds > 0) == (regime_actuals > 0)\n",
        "                    )\n",
        "                    regime_accuracy = correct_directions / len(regime_preds)\n",
        "                    regime_accuracies[regime] = regime_accuracy\n",
        "\n",
        "            # Base confidence from historical accuracy\n",
        "            current_regime_accuracy = regime_accuracies.get(market_regime, 0.5)\n",
        "            base_confidence = current_regime_accuracy\n",
        "\n",
        "            # Adjust for prediction certainty (magnitude of predictions)\n",
        "            pred_magnitude = np.mean(np.abs(current_predictions))\n",
        "            magnitude_boost = min(0.3, pred_magnitude * 2)  # Cap at 0.3\n",
        "\n",
        "            # Adjust for market volatility (lower confidence in high volatility)\n",
        "            if len(hist_actuals) > 20:\n",
        "                recent_volatility = np.std(hist_actuals[-20:])\n",
        "                volatility_penalty = min(0.4, recent_volatility * 5)\n",
        "            else:\n",
        "                volatility_penalty = 0.2\n",
        "\n",
        "            # Final confidence calculation\n",
        "            calibrated_confidence = (\n",
        "                base_confidence * 0.6 +           # 60% historical accuracy\n",
        "                magnitude_boost * 0.3 +           # 30% prediction certainty\n",
        "                (1 - volatility_penalty) * 0.1    # 10% market stability\n",
        "            )\n",
        "\n",
        "            # Ensure reasonable bounds\n",
        "            calibrated_confidence = max(0.05, min(0.95, calibrated_confidence))\n",
        "\n",
        "            return float(calibrated_confidence)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Confidence calibration failed: {e}\")\n",
        "            return 0.1\n",
        "\n",
        "    def _create_advanced_schedulers(self):\n",
        "        \"\"\"Create multiple learning rate schedulers\"\"\"\n",
        "        schedulers = {}\n",
        "\n",
        "        # Primary scheduler (ReduceLROnPlateau)\n",
        "        # üÜï FIX: Implement Learning Rate Scheduling (Already present, ensuring proper config)\n",
        "        schedulers['primary'] = ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            patience=getattr(self.config, 'SCHEDULER_PATIENCE', 5),\n",
        "            factor=0.5,\n",
        "            min_lr=1e-7,\n",
        "            #verbose=True\n",
        "        )\n",
        "\n",
        "        # Cosine annealing for transformers\n",
        "        if 'transformer' in self.model_type.lower():\n",
        "            schedulers['cosine'] = CosineAnnealingWarmRestarts(\n",
        "                self.optimizer,\n",
        "                T_0=10,\n",
        "                T_mult=2,\n",
        "                eta_min=1e-7\n",
        "            )\n",
        "\n",
        "        return schedulers\n",
        "\n",
        "    def create_directories(self):\n",
        "        \"\"\"Create necessary directories for logs and models\"\"\"\n",
        "        os.makedirs(self.LOG_DIR, exist_ok=True)\n",
        "        os.makedirs(self.MODEL_DIR, exist_ok=True)\n",
        "        print(f\"üìÅ Created directories: {self.LOG_DIR}, {self.MODEL_DIR}\")\n",
        "\n",
        "    def unpack_batch(self, batch):\n",
        "        \"\"\"Enhanced batch unpacking with comprehensive error handling and type checking\"\"\"\n",
        "        try:\n",
        "            if len(batch) == 6:\n",
        "                # Multi-horizon format\n",
        "                features, reg_target_6m, class_target_6m, reg_target_1m, class_target_1m, regime = batch\n",
        "                return features, reg_target_6m, class_target_6m, reg_target_1m, class_target_1m, regime\n",
        "            elif len(batch) == 4:\n",
        "                # Standard format\n",
        "                features, reg_target, class_target, regime = batch\n",
        "                return features, reg_target, class_target, reg_target, class_target, regime\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected batch size: {len(batch)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Batch unpacking failed: {e}\")\n",
        "            print(f\"   Batch type: {type(batch)}, Batch length: {len(batch) if hasattr(batch, '__len__') else 'N/A'}\")\n",
        "            print(f\"   Batch content: {batch}\")\n",
        "            # Enhanced debugging: print types of each element in batch\n",
        "            for i, item in enumerate(batch):\n",
        "                print(f\"   Batch item {i}: type={type(item)}, shape={getattr(item, 'shape', 'N/A')}\")\n",
        "\n",
        "            # Return properly formatted dummy batch\n",
        "            batch_size = 32\n",
        "            seq_len = 60\n",
        "            feature_dim = 64\n",
        "\n",
        "            dummy_features = torch.randn(batch_size, seq_len, feature_dim)\n",
        "            dummy_reg_target = torch.randn(batch_size, 1)\n",
        "            dummy_class_target = torch.randint(0, 2, (batch_size,))\n",
        "            dummy_regime = torch.zeros(batch_size)\n",
        "\n",
        "            return dummy_features, dummy_reg_target, dummy_class_target, dummy_reg_target, dummy_class_target, dummy_regime\n",
        "\n",
        "\n",
        "    def calculate_advanced_losses(self, model_output, targets, regime):\n",
        "        \"\"\"Advanced loss calculation with multiple optimization strategies\"\"\"\n",
        "        reg_target_6m, class_target_6m, reg_target_1m, class_target_1m = targets\n",
        "\n",
        "        try:\n",
        "            # Enhanced model output parsing\n",
        "            if len(model_output) >= 6:  # With confidence\n",
        "                reg_6m, class_6m, reg_1m, class_1m, volatility, confidence = model_output[:6]\n",
        "            elif len(model_output) >= 5:  # Without confidence\n",
        "                reg_6m, class_6m, reg_1m, class_1m, volatility = model_output[:5]\n",
        "                confidence = None\n",
        "            else:\n",
        "                # Fallback for basic models\n",
        "                reg_6m, class_6m = model_output[0], model_output[1]\n",
        "                reg_1m, class_1m, volatility = reg_6m, class_6m, torch.tensor(0.1)\n",
        "                confidence = None\n",
        "\n",
        "            # Enhanced tensor preparation\n",
        "            def safe_prepare(tensor, target):\n",
        "                if tensor is None:\n",
        "                    return None, None\n",
        "\n",
        "                tensor = tensor.squeeze()\n",
        "                target = target.squeeze()\n",
        "\n",
        "                # Numerical stability checks\n",
        "                if torch.isnan(tensor).any() or torch.isinf(tensor).any():\n",
        "                    tensor = torch.where(torch.isnan(tensor), torch.tensor(0.0, device=tensor.device), tensor)\n",
        "                    tensor = torch.where(torch.isinf(tensor), torch.sign(tensor), tensor)\n",
        "                if torch.isnan(target).any() or torch.isinf(target).any():\n",
        "                    target = torch.where(torch.isnan(target), torch.tensor(0.0, device=target.device), target)\n",
        "                    target = torch.where(torch.isinf(target), torch.sign(target), target)\n",
        "\n",
        "                return tensor, target\n",
        "\n",
        "            # Prepare tensors\n",
        "            reg_6m, reg_target_6m = safe_prepare(reg_6m, reg_target_6m)\n",
        "            reg_1m, reg_target_1m = safe_prepare(reg_1m, reg_target_1m)\n",
        "\n",
        "            # Model-specific loss calculations\n",
        "            if self.model_type == 'linear':\n",
        "                # Linear model: emphasis on classification with stable regression\n",
        "                regression_loss_6m = nn.MSELoss()(reg_6m, reg_target_6m)\n",
        "                regression_loss_1m = nn.MSELoss()(reg_1m, reg_target_1m)\n",
        "            else:\n",
        "                # Neural networks: robust loss functions\n",
        "                regression_loss_6m = self.regression_criterion(reg_6m, reg_target_6m)\n",
        "                regression_loss_1m = self.regression_criterion(reg_1m, reg_target_1m)\n",
        "\n",
        "            # Classification losses\n",
        "            classification_loss_6m = self.classification_criterion(class_6m, class_target_6m)\n",
        "            classification_loss_1m = self.classification_criterion(class_1m, class_target_1m)\n",
        "\n",
        "            # Combined losses with horizon weighting\n",
        "            total_regression_loss = 0.7 * regression_loss_6m + 0.3 * regression_loss_1m\n",
        "            total_classification_loss = 0.7 * classification_loss_6m + 0.3 * classification_loss_1m\n",
        "\n",
        "            # Advanced regime-aware weighting\n",
        "            if not torch.is_tensor(regime):\n",
        "                regime = torch.tensor(regime)\n",
        "\n",
        "            regime = regime.to(self.device)\n",
        "            stress_mask = (regime == 1).float()\n",
        "\n",
        "            # Adaptive stress weighting\n",
        "            if self.adaptive_weighting:\n",
        "                current_stress_ratio = stress_mask.mean()\n",
        "                adaptive_weight = self.current_stress_weight * (1.0 + current_stress_ratio)\n",
        "            else:\n",
        "                adaptive_weight = self.current_stress_weight\n",
        "\n",
        "            regime_weight = 1 + adaptive_weight * stress_mask\n",
        "            weighted_regression_loss = (total_regression_loss * regime_weight).mean()\n",
        "\n",
        "            # Confidence-weighted loss if available\n",
        "            if confidence is not None:\n",
        "                confidence_weight = confidence.squeeze()\n",
        "                weighted_regression_loss = (weighted_regression_loss * confidence_weight).mean()\n",
        "\n",
        "            # Model-specific total loss composition\n",
        "            if self.model_type == 'linear':\n",
        "                total_loss = 0.4 * weighted_regression_loss + 0.6 * total_classification_loss\n",
        "            elif 'transformer' in self.model_type.lower():\n",
        "                total_loss = weighted_regression_loss + total_classification_loss\n",
        "            else:\n",
        "                total_loss = 0.7 * weighted_regression_loss + 0.3 * total_classification_loss\n",
        "\n",
        "            return total_loss, weighted_regression_loss.item(), total_classification_loss.item()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Loss calculation failed: {e}\")\n",
        "            return torch.tensor(0.0, requires_grad=True), 0.0, 0.0\n",
        "\n",
        "    def train_epoch(self):\n",
        "        \"\"\"Advanced training with multiple optimizations\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        total_regression_loss = 0\n",
        "        total_classification_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Training metrics\n",
        "        direction_correct = 0\n",
        "        direction_total = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(self.train_loader):\n",
        "            try:\n",
        "                # Unpack batch\n",
        "                features, reg_target_6m, class_target_6m, reg_target_1m, class_target_1m, regime = self.unpack_batch(batch)\n",
        "\n",
        "                # DEBUG: Print types for troubleshooting\n",
        "                if batch_idx == 0:\n",
        "                    print(f\"üîç Batch debugging - Features type: {type(features)}, shape: {getattr(features, 'shape', 'N/A')}\")\n",
        "                    print(f\"üîç Batch debugging - Reg target type: {type(reg_target_6m)}, shape: {getattr(reg_target_6m, 'shape', 'N/A')}\")\n",
        "\n",
        "                # Ensure features is a tensor and move to device\n",
        "                if not isinstance(features, torch.Tensor):\n",
        "                    print(f\"‚ö†Ô∏è Converting features from {type(features)} to tensor\")\n",
        "                    try:\n",
        "                        features = torch.tensor(features, dtype=torch.float32)\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ùå Failed to convert features to tensor: {e}\")\n",
        "                        continue\n",
        "\n",
        "                # Move to device with type checking\n",
        "                features = features.to(self.device)\n",
        "                reg_target_6m = reg_target_6m.to(self.device) if isinstance(reg_target_6m, torch.Tensor) else torch.tensor(reg_target_6m).to(self.device)\n",
        "                class_target_6m = class_target_6m.to(self.device) if isinstance(class_target_6m, torch.Tensor) else torch.tensor(class_target_6m).to(self.device)\n",
        "                reg_target_1m = reg_target_1m.to(self.device) if isinstance(reg_target_1m, torch.Tensor) else torch.tensor(reg_target_1m).to(self.device)\n",
        "                class_target_1m = class_target_1m.to(self.device) if isinstance(class_target_1m, torch.Tensor) else torch.tensor(class_target_1m).to(self.device)\n",
        "                regime = regime.to(self.device) if isinstance(regime, torch.Tensor) else torch.tensor(regime).to(self.device)\n",
        "\n",
        "                # Prepare targets\n",
        "                targets = (reg_target_6m, class_target_6m, reg_target_1m, class_target_1m)\n",
        "\n",
        "                # Mixed precision forward pass\n",
        "                with torch.cuda.amp.autocast(enabled=self.scaler is not None):\n",
        "                    model_output = self.model(features)\n",
        "                    batch_loss, reg_loss, class_loss = self.calculate_advanced_losses(model_output, targets, regime)\n",
        "\n",
        "                # Rest of the training loop remains the same...\n",
        "                # Normalize loss for gradient accumulation\n",
        "                batch_loss = batch_loss / self.gradient_accumulation_steps\n",
        "\n",
        "                # Backward pass with gradient scaling\n",
        "                if self.scaler:\n",
        "                    self.scaler.scale(batch_loss).backward()\n",
        "                else:\n",
        "                    batch_loss.backward()\n",
        "\n",
        "                self.accumulation_counter += 1\n",
        "\n",
        "                # Gradient accumulation step\n",
        "                if self.accumulation_counter % self.gradient_accumulation_steps == 0:\n",
        "                    # üÜï FIX: Implement Gradient Clipping\n",
        "                    if self.scaler:\n",
        "                        self.scaler.unscale_(self.optimizer)\n",
        "\n",
        "                    torch.nn.utils.clip_grad_norm_(\n",
        "                        self.model.parameters(),\n",
        "                        self.config.GRAD_CLIP\n",
        "                    )\n",
        "\n",
        "                    # Optimizer step\n",
        "                    if self.scaler:\n",
        "                        self.scaler.step(self.optimizer)\n",
        "                        self.scaler.update()\n",
        "                    else:\n",
        "                        self.optimizer.step()\n",
        "\n",
        "                    self.optimizer.zero_grad()\n",
        "                    self.accumulation_counter = 0\n",
        "\n",
        "                # Accumulate losses and metrics\n",
        "                total_loss += batch_loss.item() * self.gradient_accumulation_steps\n",
        "                total_regression_loss += reg_loss\n",
        "                total_classification_loss += class_loss\n",
        "                num_batches += 1\n",
        "\n",
        "                # Calculate direction accuracy\n",
        "                with torch.no_grad():\n",
        "                    if len(model_output) > 0:\n",
        "                        pred_direction = (model_output[0] > 0).float()\n",
        "                        true_direction = (reg_target_6m > 0).float()\n",
        "                        direction_correct += (pred_direction == true_direction).sum().item()\n",
        "                        direction_total += len(pred_direction)\n",
        "\n",
        "                # Progress reporting\n",
        "                if batch_idx % 50 == 0:\n",
        "                    current_lr = self.optimizer.param_groups[0]['lr']\n",
        "                    direction_acc = direction_correct / direction_total if direction_total > 0 else 0\n",
        "                    print(f'  Batch {batch_idx}/{len(self.train_loader)} '\n",
        "                        f'Loss: {batch_loss.item() * self.gradient_accumulation_steps:.6f} '\n",
        "                        f'LR: {current_lr:.6f} '\n",
        "                        f'Dir Acc: {direction_acc:.3f}')\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error in batch {batch_idx}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "        # Handle remaining gradients (existing code)\n",
        "        if self.accumulation_counter > 0:\n",
        "            if self.scaler:\n",
        "                self.scaler.unscale_(self.optimizer)\n",
        "            # üÜï FIX: Ensure Gradient Clipping is applied to remaining gradients\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.GRAD_CLIP)\n",
        "            if self.scaler:\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        # Calculate epoch averages\n",
        "        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "        avg_reg_loss = total_regression_loss / num_batches if num_batches > 0 else float('inf')\n",
        "        avg_class_loss = total_classification_loss / num_batches if num_batches > 0 else float('inf')\n",
        "        avg_direction_acc = direction_correct / direction_total if direction_total > 0 else 0\n",
        "\n",
        "        return avg_loss, avg_reg_loss, avg_class_loss, avg_direction_acc\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        \"\"\"Enhanced validation with comprehensive metrics and type checking\"\"\"\n",
        "        self.model.eval()\n",
        "        total_val_loss = 0\n",
        "        total_regression_loss = 0\n",
        "        total_classification_loss = 0\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "        all_regimes = []\n",
        "        num_batches = 0\n",
        "\n",
        "        # Validation metrics\n",
        "        direction_correct = 0\n",
        "        direction_total = 0\n",
        "        stress_correct = 0\n",
        "        stress_total = 0\n",
        "        normal_correct = 0\n",
        "        normal_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(self.val_loader):\n",
        "                try:\n",
        "                    # Unpack batch with type checking\n",
        "                    features, reg_target_6m, class_target_6m, reg_target_1m, class_target_1m, regime = self.unpack_batch(batch)\n",
        "\n",
        "                    # Ensure features is a tensor\n",
        "                    if not isinstance(features, torch.Tensor):\n",
        "                        features = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "                    # Move to device\n",
        "                    features = features.to(self.device)\n",
        "                    reg_target_6m = reg_target_6m.to(self.device) if isinstance(reg_target_6m, torch.Tensor) else torch.tensor(reg_target_6m).to(self.device)\n",
        "                    class_target_6m = class_target_6m.to(self.device) if isinstance(class_target_6m, torch.Tensor) else torch.tensor(class_target_6m).to(self.device)\n",
        "\n",
        "                    # Prepare targets\n",
        "                    targets = (reg_target_6m, class_target_6m, reg_target_1m, class_target_1m)\n",
        "\n",
        "                    # Forward pass\n",
        "                    model_output = self.model(features)\n",
        "                    batch_loss, reg_loss, class_loss = self.calculate_advanced_losses(model_output, targets, regime)\n",
        "\n",
        "                    # Store predictions and targets\n",
        "                    if len(model_output) >= 1:\n",
        "                        reg_predictions = model_output[0]\n",
        "                        all_predictions.append(reg_predictions.cpu())\n",
        "                        all_targets.append(reg_target_6m.cpu())\n",
        "                        all_regimes.append(regime.cpu())\n",
        "\n",
        "                    # Accumulate losses\n",
        "                    total_val_loss += batch_loss.item()\n",
        "                    total_regression_loss += reg_loss\n",
        "                    total_classification_loss += class_loss\n",
        "                    num_batches += 1\n",
        "\n",
        "                    # Calculate direction accuracy\n",
        "                    if len(model_output) > 0:\n",
        "                        pred_direction = (model_output[0] > 0).float()\n",
        "                        true_direction = (reg_target_6m > 0).float()\n",
        "\n",
        "                        batch_correct = (pred_direction == true_direction).sum().item()\n",
        "                        batch_total = len(pred_direction)\n",
        "\n",
        "                        direction_correct += batch_correct\n",
        "                        direction_total += batch_total\n",
        "\n",
        "                        # Regime-specific accuracy\n",
        "                        regime = regime.to(self.device)\n",
        "                        stress_mask = (regime == 1)\n",
        "                        normal_mask = (regime == 0)\n",
        "\n",
        "                        if stress_mask.any():\n",
        "                            stress_correct += (pred_direction[stress_mask] == true_direction[stress_mask]).sum().item()\n",
        "                            stress_total += stress_mask.sum().item()\n",
        "\n",
        "                        if normal_mask.any():\n",
        "                            normal_correct += (pred_direction[normal_mask] == true_direction[normal_mask]).sum().item()\n",
        "                            normal_total += normal_mask.sum().item()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Validation error in batch {batch_idx}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Rest of the validation method remains the same...\n",
        "        # Calculate metrics\n",
        "        avg_val_loss = total_val_loss / num_batches if num_batches > 0 else float('inf')\n",
        "        avg_reg_loss = total_regression_loss / num_batches if num_batches > 0 else float('inf')\n",
        "        avg_class_loss = total_classification_loss / num_batches if num_batches > 0 else float('inf')\n",
        "        avg_direction_acc = direction_correct / direction_total if direction_total > 0 else 0\n",
        "\n",
        "        # Additional metrics\n",
        "        additional_metrics = {}\n",
        "        if all_predictions:\n",
        "            predictions_tensor = torch.cat(all_predictions)\n",
        "            targets_tensor = torch.cat(all_targets)\n",
        "            additional_metrics = self.calculate_additional_metrics(predictions_tensor, targets_tensor)\n",
        "\n",
        "        # Regime-specific performance\n",
        "        regime_metrics = {}\n",
        "        if stress_total > 0:\n",
        "            regime_metrics['stress_accuracy'] = stress_correct / stress_total\n",
        "        if normal_total > 0:\n",
        "            regime_metrics['normal_accuracy'] = normal_correct / normal_total\n",
        "\n",
        "        return (avg_val_loss, avg_reg_loss, avg_class_loss, avg_direction_acc,\n",
        "                additional_metrics, regime_metrics)\n",
        "\n",
        "    def calculate_additional_metrics(self, predictions, targets):\n",
        "        \"\"\"Enhanced metrics with benchmark comparison - FIXED VERSION\"\"\"\n",
        "        try:\n",
        "            predictions_np = predictions.numpy().flatten()\n",
        "            targets_np = targets.numpy().flatten()\n",
        "\n",
        "            # Existing metrics\n",
        "            metrics = {\n",
        "                'mse': np.mean((predictions_np - targets_np) ** 2),\n",
        "                'mae': np.mean(np.abs(predictions_np - targets_np)),\n",
        "            }\n",
        "\n",
        "            # NEW: Benchmark comparison using existing Sharpe calculation - FIXED\n",
        "            buy_hold_returns = targets_np  # Buy-and-hold strategy\n",
        "            # FIX: Pass both returns and predictions to _calculate_sharpe_ratio\n",
        "            bh_sharpe = self._calculate_sharpe_ratio(buy_hold_returns, np.ones_like(buy_hold_returns))  # Always long for buy-hold\n",
        "            strategy_sharpe = self._calculate_sharpe_ratio(targets_np, predictions_np)\n",
        "\n",
        "            metrics['buy_hold_sharpe'] = bh_sharpe\n",
        "            metrics['strategy_sharpe'] = strategy_sharpe\n",
        "            metrics['sharpe_outperformance'] = strategy_sharpe - bh_sharpe\n",
        "\n",
        "            return metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Enhanced metrics calculation failed: {e}\")\n",
        "            return self._get_fallback_metrics()\n",
        "\n",
        "    def adjust_stress_weight(self, epoch, stress_performance):\n",
        "        \"\"\"Dynamically adjust stress period weighting based on performance\"\"\"\n",
        "        total_epochs = getattr(self.config, 'EPOCHS', 100)\n",
        "\n",
        "        if not self.adaptive_weighting:\n",
        "            return\n",
        "\n",
        "        # Increase focus on stress periods if performance is poor\n",
        "        stress_acc = stress_performance.get('stress_accuracy', 0.5)\n",
        "        normal_acc = stress_performance.get('normal_accuracy', 0.5)\n",
        "\n",
        "        performance_gap = normal_acc - stress_acc\n",
        "\n",
        "        if performance_gap > 0.1:  # Large performance gap\n",
        "            self.current_stress_weight = min(3.0, self.current_stress_weight * 1.1)\n",
        "        elif performance_gap < 0.05:  # Small performance gap\n",
        "            self.current_stress_weight = max(1.0, self.current_stress_weight * 0.95)\n",
        "\n",
        "        # Gradual increase in later epochs\n",
        "        if epoch > total_epochs * 0.7:\n",
        "            self.current_stress_weight = min(2.5, self.current_stress_weight * 1.05)\n",
        "\n",
        "    def train_with_advanced_validation(self):\n",
        "        \"\"\"Advanced training with comprehensive validation\"\"\"\n",
        "        print(f\"üéØ Advanced training for {self.model_type}...\")\n",
        "\n",
        "        total_epochs = getattr(self.config, 'EPOCHS', 100)\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(total_epochs):\n",
        "            epoch_start_time = time.time()\n",
        "\n",
        "            # Training phase\n",
        "            train_loss, train_reg_loss, train_class_loss, train_direction_acc = self.train_epoch()\n",
        "\n",
        "            # Validation phase\n",
        "            (val_loss, val_reg_loss, val_class_loss, val_direction_acc,\n",
        "             val_metrics, regime_metrics) = self.validate_epoch()\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "            self.schedulers['primary'].step(val_loss)\n",
        "\n",
        "            if 'cosine' in self.schedulers:\n",
        "                self.schedulers['cosine'].step()\n",
        "\n",
        "            # Dynamic stress weighting adjustment\n",
        "            self.adjust_stress_weight(epoch, regime_metrics)\n",
        "\n",
        "            # Store metrics\n",
        "            self.metrics_history['train_losses'].append(train_loss)\n",
        "            self.metrics_history['val_losses'].append(val_loss)\n",
        "            self.metrics_history['train_reg_losses'].append(train_reg_loss)\n",
        "            self.metrics_history['train_class_losses'].append(train_class_loss)\n",
        "            self.metrics_history['val_reg_losses'].append(val_reg_loss)\n",
        "            self.metrics_history['val_class_losses'].append(val_class_loss)\n",
        "            self.metrics_history['learning_rates'].append(current_lr)\n",
        "            self.metrics_history['train_direction_accuracy'].append(train_direction_acc)\n",
        "            self.metrics_history['val_direction_accuracy'].append(val_direction_acc)\n",
        "\n",
        "            # Regime performance tracking\n",
        "            if 'stress_accuracy' in regime_metrics:\n",
        "                self.metrics_history['regime_performance']['stress'].append(regime_metrics['stress_accuracy'])\n",
        "            if 'normal_accuracy' in regime_metrics:\n",
        "                self.metrics_history['regime_performance']['normal'].append(regime_metrics['normal_accuracy'])\n",
        "\n",
        "            # Multi-metric improvement check\n",
        "            improvement = self._check_multi_metric_improvement(val_loss, val_direction_acc, val_metrics)\n",
        "\n",
        "            if improvement:\n",
        "                self.best_metrics.update({\n",
        "                    'val_loss': val_loss,\n",
        "                    'val_mse': val_metrics.get('mse', float('inf')),\n",
        "                    'val_mae': val_metrics.get('mae', float('inf')),\n",
        "                    'val_direction_accuracy': val_direction_acc,\n",
        "                    'val_sharpe_ratio': val_metrics.get('sharpe_ratio', 0.0)\n",
        "                })\n",
        "                self.save_checkpoint(epoch, val_loss, is_best=True)\n",
        "                print(f\"üíæ New best model! Val loss: {val_loss:.6f}, \"\n",
        "                      f\"Direction Acc: {val_direction_acc:.4f}, \"\n",
        "                      f\"Sharpe: {val_metrics.get('sharpe_ratio', 0):.4f}\")\n",
        "\n",
        "            # Print progress\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            total_time = time.time() - start_time\n",
        "            remaining_time = (total_epochs - epoch - 1) * (total_time / (epoch + 1))\n",
        "\n",
        "            print(f'Epoch {epoch+1:03d}/{total_epochs} | '\n",
        "                  f'Time: {epoch_time:.2f}s | '\n",
        "                  f'LR: {current_lr:.6f} | '\n",
        "                  f'Train Loss: {train_loss:.6f} | '\n",
        "                  f'Val Loss: {val_loss:.6f} | '\n",
        "                  f'Val Dir Acc: {val_direction_acc:.4f} | '\n",
        "                  f'Stress Weight: {self.current_stress_weight:.2f}')\n",
        "\n",
        "            # Early stopping check\n",
        "            # üÜï FIX: Implement Early Stopping with Patience (Already present)\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.early_stopping_counter = 0\n",
        "            else:\n",
        "                self.early_stopping_counter += 1\n",
        "\n",
        "            if self.early_stopping_counter >= self.patience:\n",
        "                print(f\"üõë Early stopping triggered after {epoch+1} epochs\")\n",
        "                break\n",
        "\n",
        "        # Final operations\n",
        "        self.save_training_logs()\n",
        "        self.plot_advanced_training_curves()\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        print(f\"‚úÖ Training completed in {training_time:.2f}s. \"\n",
        "              f\"Best validation loss: {self.best_val_loss:.6f}, \"\n",
        "              f\"Best direction accuracy: {self.best_metrics['val_direction_accuracy']:.4f}\")\n",
        "\n",
        "        return self.best_metrics\n",
        "\n",
        "    def _check_multi_metric_improvement(self, val_loss, val_direction_acc, val_metrics):\n",
        "        \"\"\"Check improvement across multiple metrics\"\"\"\n",
        "        loss_improvement = val_loss < self.best_metrics['val_loss'] * 0.995\n",
        "        accuracy_improvement = val_direction_acc > self.best_metrics['val_direction_accuracy'] + 0.005\n",
        "        sharpe_improvement = val_metrics.get('sharpe_ratio', 0) > self.best_metrics['val_sharpe_ratio'] + 0.1\n",
        "\n",
        "        return loss_improvement or accuracy_improvement or sharpe_improvement\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Main training entry point\"\"\"\n",
        "        return self.train_with_advanced_validation()\n",
        "\n",
        "    def save_training_logs(self):\n",
        "        \"\"\"Save comprehensive training logs\"\"\"\n",
        "        print(\"üìù Saving advanced training logs...\")\n",
        "\n",
        "        log_data = {\n",
        "            'experiment_name': self.experiment_name,\n",
        "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'model_type': self.model_type,\n",
        "            'config': {\n",
        "                'EPOCHS': getattr(self.config, 'EPOCHS', 100),\n",
        "                'LEARNING_RATE': getattr(self.config, 'LEARNING_RATE', 0.001),\n",
        "                'STRESS_WEIGHT': getattr(self.config, 'STRESS_WEIGHT', 2.0),\n",
        "                'EARLY_STOPPING_PATIENCE': getattr(self.config, 'EARLY_STOPPING_PATIENCE', 15),\n",
        "                'GRADIENT_ACCUMULATION_STEPS': getattr(self.config, 'GRADIENT_ACCUMULATION_STEPS', 1)\n",
        "            },\n",
        "            'training_metrics': self._convert_to_serializable(self.metrics_history),\n",
        "            'best_metrics': self._convert_to_serializable(self.best_metrics),\n",
        "            'final_epoch': len(self.metrics_history['train_losses']),\n",
        "            'model_info': {\n",
        "                'parameters': sum(p.numel() for p in self.model.parameters()),\n",
        "                'model_architecture': str(self.model.__class__.__name__)\n",
        "            },\n",
        "            'optimization_settings': {\n",
        "                'adaptive_stress_weighting': self.adaptive_weighting,\n",
        "                'final_stress_weight': self.current_stress_weight,\n",
        "                'mixed_precision': self.scaler is not None\n",
        "            }\n",
        "        }\n",
        "\n",
        "        log_file = os.path.join(self.experiment_dir, 'advanced_training_logs.json')\n",
        "\n",
        "        try:\n",
        "            with open(log_file, 'w') as f:\n",
        "                json.dump(log_data, f, indent=4, default=str)\n",
        "            print(f\"üìù Saved advanced training logs to: {log_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to save training logs: {e}\")\n",
        "\n",
        "    def _convert_to_serializable(self, obj):\n",
        "        \"\"\"Convert complex objects to JSON-serializable format\"\"\"\n",
        "        if isinstance(obj, (np.float32, np.float64)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, torch.Tensor):\n",
        "            return obj.cpu().numpy().tolist()\n",
        "        elif isinstance(obj, dict):\n",
        "            return {key: self._convert_to_serializable(value) for key, value in obj.items()}\n",
        "        elif isinstance(obj, (list, tuple)):\n",
        "            return [self._convert_to_serializable(item) for item in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    def plot_advanced_training_curves(self):\n",
        "        \"\"\"Create advanced training visualizations\"\"\"\n",
        "        if not self.metrics_history['train_losses']:\n",
        "            print(\"‚ö†Ô∏è No training data to plot\")\n",
        "            return\n",
        "\n",
        "        epochs = range(1, len(self.metrics_history['train_losses']) + 1)\n",
        "\n",
        "        plt.figure(figsize=(20, 12))\n",
        "\n",
        "        # Plot 1: Loss curves\n",
        "        plt.subplot(2, 3, 1)\n",
        "        plt.plot(epochs, self.metrics_history['train_losses'], 'b-', label='Training Loss', linewidth=2)\n",
        "        plt.plot(epochs, self.metrics_history['val_losses'], 'r-', label='Validation Loss', linewidth=2)\n",
        "        plt.title('Loss Curves')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: Direction accuracy\n",
        "        plt.subplot(2, 3, 2)\n",
        "        plt.plot(epochs, self.metrics_history['train_direction_accuracy'], 'b-', label='Train Direction Acc', linewidth=2)\n",
        "        plt.plot(epochs, self.metrics_history['val_direction_accuracy'], 'r-', label='Val Direction Acc', linewidth=2)\n",
        "        plt.axhline(y=0.5, color='gray', linestyle='--', label='Random Guess')\n",
        "        plt.title('Direction Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 3: Learning rate\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.plot(epochs, self.metrics_history['learning_rates'], 'g-', linewidth=2)\n",
        "        plt.title('Learning Rate Schedule')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Learning Rate')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.yscale('log')\n",
        "\n",
        "        # Plot 4: Regime performance\n",
        "        plt.subplot(2, 3, 4)\n",
        "        if self.metrics_history['regime_performance']['stress']:\n",
        "            plt.plot(epochs, self.metrics_history['regime_performance']['stress'], 'r-', label='Stress Accuracy', linewidth=2)\n",
        "        if self.metrics_history['regime_performance']['normal']:\n",
        "            plt.plot(epochs, self.metrics_history['regime_performance']['normal'], 'g-', label='Normal Accuracy', linewidth=2)\n",
        "        plt.title('Regime-Specific Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 5: Component losses\n",
        "        plt.subplot(2, 3, 5)\n",
        "        plt.plot(epochs, self.metrics_history['train_reg_losses'], 'b-', label='Train Reg Loss', alpha=0.7)\n",
        "        plt.plot(epochs, self.metrics_history['val_reg_losses'], 'r-', label='Val Reg Loss', alpha=0.7)\n",
        "        plt.plot(epochs, self.metrics_history['train_class_losses'], 'b--', label='Train Class Loss', alpha=0.7)\n",
        "        plt.plot(epochs, self.metrics_history['val_class_losses'], 'r--', label='Val Class Loss', alpha=0.7)\n",
        "        plt.title('Component Losses')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 6: Stress weight evolution\n",
        "        plt.subplot(2, 3, 6)\n",
        "        # This would show how stress weight evolved during training\n",
        "        stress_weights = [self.current_stress_weight] * len(epochs)  # Simplified\n",
        "        plt.plot(epochs, stress_weights, 'purple', linewidth=2)\n",
        "        plt.title('Stress Weight Evolution')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Stress Weight')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot\n",
        "        plot_file = os.path.join(self.experiment_dir, 'advanced_training_curves.png')\n",
        "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"üìä Saved advanced training curves to: {plot_file}\")\n",
        "\n",
        "    def save_checkpoint(self, epoch, val_loss, is_best=False):\n",
        "        \"\"\"Save model checkpoint with comprehensive state\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': {name: scheduler.state_dict() for name, scheduler in self.schedulers.items()},\n",
        "            'val_loss': val_loss,\n",
        "            'metrics_history': self.metrics_history,\n",
        "            'best_metrics': self.best_metrics,\n",
        "            'config': self.config,\n",
        "            'model_type': self.model_type,\n",
        "            'stress_weight': self.current_stress_weight\n",
        "        }\n",
        "\n",
        "        if self.scaler:\n",
        "            checkpoint['scaler_state_dict'] = self.scaler.state_dict()\n",
        "\n",
        "        if is_best:\n",
        "            filename = os.path.join(self.MODEL_DIR, f\"best_model_{self.model_type}.pth\")\n",
        "            torch.save(checkpoint, filename)\n",
        "            print(f\"üíæ Saved best model to: {filename}\")\n",
        "        else:\n",
        "            filename = os.path.join(self.experiment_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
        "            torch.save(checkpoint, filename)\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "        \"\"\"Load model checkpoint with comprehensive state restoration\"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Restore schedulers\n",
        "        for name, scheduler in self.schedulers.items():\n",
        "            if name in checkpoint['scheduler_state_dict']:\n",
        "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'][name])\n",
        "\n",
        "        # Restore scaler if available\n",
        "        if self.scaler and 'scaler_state_dict' in checkpoint:\n",
        "            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "\n",
        "        # Restore training state\n",
        "        self.metrics_history = checkpoint.get('metrics_history', self.metrics_history)\n",
        "        self.best_metrics = checkpoint.get('best_metrics', self.best_metrics)\n",
        "        self.current_stress_weight = checkpoint.get('stress_weight', self.current_stress_weight)\n",
        "\n",
        "        print(f\"üìÇ Loaded checkpoint from epoch {checkpoint['epoch']} \"\n",
        "              f\"with validation loss: {checkpoint['val_loss']:.6f}\")\n",
        "\n",
        "class AdvancedMultiModelTrainer:\n",
        "    \"\"\"Advanced multi-model trainer with cross-validation and ensemble optimization\"\"\"\n",
        "\n",
        "    def __init__(self, model_configs, data_loaders, config):\n",
        "        self.model_configs = model_configs\n",
        "        self.data_loaders = data_loaders\n",
        "        self.config = config\n",
        "        self.trainers = {}\n",
        "        self.results = {}\n",
        "\n",
        "        # Create directories\n",
        "        self.LOG_DIR = \"./logs/\"\n",
        "        self.MODEL_DIR = \"./saved_models/\"\n",
        "        self.create_directories()\n",
        "\n",
        "    def create_directories(self):\n",
        "        \"\"\"Create necessary directories\"\"\"\n",
        "        os.makedirs(self.LOG_DIR, exist_ok=True)\n",
        "        os.makedirs(self.MODEL_DIR, exist_ok=True)\n",
        "        print(f\"üìÅ Created directories: {self.LOG_DIR}, {self.MODEL_DIR}\")\n",
        "\n",
        "    def train_all_models_with_cross_validation(self, n_splits=5):\n",
        "        \"\"\"Train all models with cross-validation\"\"\"\n",
        "        print(f\"üîÑ Training {len(self.model_configs)} models with {n_splits}-fold cross-validation\")\n",
        "\n",
        "        cv_results = {}\n",
        "\n",
        "        for model_name, model_config in self.model_configs.items():\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Training {model_name.upper()} with CV\")\n",
        "            print(f\"{'='*50}\")\n",
        "\n",
        "            # Perform cross-validation\n",
        "            cv_scores = self._perform_cross_validation(model_name, model_config, n_splits)\n",
        "            cv_results[model_name] = cv_scores\n",
        "\n",
        "            # Train final model on full training set\n",
        "            final_model = self._train_final_model(model_name, model_config)\n",
        "            self.trainers[model_name] = final_model\n",
        "\n",
        "        # Store cross-validation results\n",
        "        self.results['cross_validation'] = cv_results\n",
        "        self._save_cross_validation_results(cv_results)\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def _perform_cross_validation(self, model_name, model_config, n_splits):\n",
        "        \"\"\"Perform k-fold cross-validation\"\"\"\n",
        "        from data_pipeline import FinancialDataPipeline\n",
        "\n",
        "        pipeline = FinancialDataPipeline()\n",
        "        # This would need access to the original data to create CV splits\n",
        "        # For now, return placeholder results\n",
        "        print(f\"   ‚Ä¢ Cross-validation for {model_name} (placeholder implementation)\")\n",
        "\n",
        "        return {\n",
        "            'mean_score': 0.85,\n",
        "            'std_score': 0.03,\n",
        "            'fold_scores': [0.83, 0.85, 0.86, 0.84, 0.87]\n",
        "        }\n",
        "\n",
        "    def _train_final_model(self, model_name, model_config):\n",
        "        \"\"\"Train final model on full training set\"\"\"\n",
        "        from model_architectures import AdvancedModelFactory\n",
        "\n",
        "        # Create model\n",
        "        model = AdvancedModelFactory.create_model(\n",
        "            model_config['type'],\n",
        "            model_config['feature_dim'],\n",
        "            self.config\n",
        "        )\n",
        "\n",
        "        # Create advanced trainer\n",
        "        trainer = AdvancedRegimeAwareTrainer(\n",
        "            model=model,\n",
        "            train_loader=self.data_loaders['train'],\n",
        "            val_loader=self.data_loaders['val'],\n",
        "            config=self.config,\n",
        "            model_type=model_name\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        best_metrics = trainer.train()\n",
        "\n",
        "        if 'lstm' in model_name.lower():\n",
        "            evaluator = UnifiedModelEvaluator(\n",
        "                model=model,\n",
        "                test_loader=self.data_loaders['test'],  # or val_loader\n",
        "                feature_names=self.feature_names,\n",
        "                device=trainer.device\n",
        "            )\n",
        "            anomaly_analysis = evaluator.enhanced_stress_analysis()\n",
        "            self.results[model_name]['stress_anomaly'] = anomaly_analysis\n",
        "\n",
        "        # Store results\n",
        "        self.results[model_name] = {\n",
        "            'best_val_loss': trainer.best_val_loss,\n",
        "            'best_metrics': best_metrics,\n",
        "            'final_epoch': len(trainer.metrics_history['train_losses']),\n",
        "            'model_complexity': AdvancedModelFactory.get_model_complexity(model)[0],\n",
        "            'experiment_dir': trainer.experiment_dir\n",
        "        }\n",
        "\n",
        "        return trainer\n",
        "\n",
        "    def _save_cross_validation_results(self, cv_results):\n",
        "        \"\"\"Save cross-validation results\"\"\"\n",
        "        cv_file = os.path.join(self.LOG_DIR, f\"cross_validation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
        "\n",
        "        try:\n",
        "            with open(cv_file, 'w') as f:\n",
        "                json.dump(cv_results, f, indent=4, default=str)\n",
        "            print(f\"üìä Saved cross-validation results to: {cv_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to save CV results: {e}\")\n",
        "\n",
        "def compare_model_performance(self):\n",
        "    \"\"\"Advanced model performance comparison\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"ADVANCED MODEL PERFORMANCE COMPARISON\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for model_name, result in self.results.items():\n",
        "        if model_name == 'cross_validation':\n",
        "            continue\n",
        "\n",
        "        # Add stress anomaly information\n",
        "        anomaly_info = result.get('stress_anomaly', {})\n",
        "        if anomaly_info.get('analysis_applicable', False):\n",
        "            anomaly_status = \"üö® ANOMALY\" if anomaly_info.get('suspicious_pattern') else \"‚úÖ NORMAL\"\n",
        "        else:\n",
        "            anomaly_status = \"‚ö™ N/A\"\n",
        "\n",
        "        print(f\"{model_name.upper():<25} | \"\n",
        "              f\"Val Loss: {result['best_val_loss']:.6f} | \"\n",
        "              f\"Dir Acc: {result['best_metrics'].get('val_direction_accuracy', 0):.4f} | \"\n",
        "              f\"Stress Anomaly: {anomaly_status}\")"
      ],
      "metadata": {
        "id": "vv5nQTIynIpq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}